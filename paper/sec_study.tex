% vim:syntax=tex

In this section we describe the design of a study in which we compare our new
methodology with the current practice. We describe the case study using the
Goal-Question-Metric approach~\cite{Basili-etal_1994}. We discuss the results of
using LDA as our topic modeler, and exclude the LSI discussion for brevity.
Further, the data and source code for the full case study is available in this
paper's online
appendix\footnote{\url{http://christop.club/publications/data/Corley-etal_2015}}.

\subsection{Definition and Context}

Our \textit{goal} is to evaluate the effectiveness of TM-based FLTs trained on
changesets. The \textit{quality focus} of the study is on informing development
decisions and policy changes that could lead to software with fewer defects.
The \textit{perspective} of the study is of a researcher, developer, or project
manager who wishes to gain understanding of the concepts or features implemented
in the source code. The \textit{context} of the study spans the version
histories of 14 open source systems.

%\todo{r3: reformulate the RQs}

Toward the achievement of our goal, we pose the following research questions:
\begin{description}[font=\itshape\mdseries,leftmargin=10mm,style=sameline]
    \item[RQ1] Is a changeset-based FLT as accurate as a snapshot-based FLT?
\end{description}

With our new methodology, we also gain the opportunity to simulate how the FLT
would perform in a real development environment, an evaluation technique not
previously feasible due to the run-time of the experiment.

\begin{description}[font=\itshape\mdseries,leftmargin=10mm,style=sameline]
    %\item[RQ2] Are FLTs accurately evaluated in realistic contexts?
    \item[RQ2] Does the accuracy of a changeset-based FLT fluctuate as a project evolves?
\end{description}

At a high level, our goal is to determine the feasibility of using changesets to
train topic models for feature location, especially in realistic development
scenarios.

In the remainder of this section we introduce the subjects of our study,
describe the setting of our study, our data collection and analysis procedures,
and report the results of the study using LDA.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Subject Software Systems}

Each of our subject software systems come from two publicly-available datasets.
The first is a dataset of six software systems by Dit et
al.~\cite{Dit-etal_2013} and contains method-level goldsets.  This dataset was
automatically extracted from changesets that relate to the queries (issue
reports). The second is a dataset of 14 software systems by Moreno et
al.~\cite{Moreno-etal_2014} and contains class-level goldsets.  The six software
systems in the first dataset also appear in the second, supplying us with both
class- and method-level goldsets for the queries.  We only consider the systems
where the datasets overlap.

\begin{table}[t]
\vspace{2mm}
\renewcommand{\arraystretch}{1.3}
\centering
\caption{Subject Systems and Goldset Sizes}
\begin{tabular}{lrrr}
    \toprule
    Subject System     & Features & Classes & Methods \\
    \midrule
    ArgoUML v0.22      & 91       & 287     & 701     \\
    ArgoUML v0.24      & 52       & 154     & 357     \\
    ArgoUML v0.26.2    & 209      & 706     & 1560    \\
    Jabref v2.6        & 39       & 131     & 280     \\
    jEdit v4.3         & 150      & 361     & 748     \\
    muCommander v0.8.5 & 92       & 303     & 717     \\
    \midrule
    Total              & 633      & 1942    & 4363    \\
    \bottomrule
\end{tabular}
\label{table:subjects}
\end{table}


%\todo{R1: add words to make this less garbage-y}
ArgoUML is a UML diagramming tool\footnote{\url{http://argouml.tigris.org/}}.
jEdit is a text editor\footnote{\url{http://www.jedit.org/}}.
JabRef is a BibTeX bibliography management tool\footnote{\url{http://jabref.sourceforge.net/}}.
muCommander is a cross-platform file manager\footnote{\url{http://www.mucommander.com/}}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Methodology}
\label{sec:methodology}

For snapshots, the process is straightforward and corresponds to
Figure~\ref{fig:snapshot}. First, we train a model on the snapshot corpus using
batch training. That is, the model can see all documents in the corpus at once.
Then, we infer an index of topic distributions with the snapshot corpus. For
each query in the dataset, we infer the query's topic distribution and rank each
entity in the index with pairwise comparisons.

\begin{comment}
\begin{enumerate}
    \item Build model from the snapshot corpus in batch mode
    \item Infer a $\theta_{Snapshot}$ from the snapshot corpus
    \item Infer a $\theta_{Queries}$ from the query corpus
    \item Classify, or rank, the results from both $\theta$s
\end{enumerate}
\end{comment}


In terms of changesets, the process varies slightly from a snapshot approach, as
shown in Figure~\ref{fig:changeset}.  First, we train a model on the changeset
corpus using batch training.  Second, we infer an index of topic distributions
with the snapshot corpus.  Note that we \emph{do not} infer topic distributions
with the changeset corpus on which the model was built.  Finally, for each query
in the dataset, we infer the query's topic distribution and rank each entity in
the snapshot index with pairwise comparisons.

\begin{comment}
\begin{enumerate}
    \item Build model from the changeset corpus in batch mode
    \item \emph{Do not} infer a $\theta_{Changesets}$
    \item Infer a $\theta_{Snapshot}$ from the snapshot corpus
    \item Infer a  $\theta_{Queries}$ from the query corpus
    \item Classify, or rank, the results from both $\theta$s
\end{enumerate}
\end{comment}

%\todo{r3: found this confusing}
%\todo{r3: why do partitioning as such?}
For the historical simulation, we take a slightly different approach. We first
determine which commits relate to each query (or issue) and partition
mini-batches out of the changesets.  We then proceed by initializing a model for
online training. Using each mini-batch, or partition, we update the model.
Then, we infer an index of topic distributions with the snapshot corpus at the
commit the partition ends on.  We also obtain a topic distribution for each
query related to the commit.  For each query, we infer the query's topic
distribution and rank each entity in the snapshot index with pairwise
comparisons.  Finally, we continue by updating the model with the next
mini-batch.

\begin{comment}
\begin{enumerate}
    \item Initialize a model in online mode
    \item Determine which changesets relate to an issue and partition mini-batches out of the changesets
    \item For each mini-batch:
        \begin{enumerate}
            \item Update the model with mini-batch
            \item Update $\theta_{Snapshot}$ with the new inference of the source code document affected by this changeset
            \item Infer a $\theta_{Query}$ of the query related to the changeset we stopped at
            \item Classify, or rank, the results from both $\theta$s
        \end{enumerate}
\end{enumerate}
\end{comment}

%\todo{R1: found this confusing}
Since the Dit et al. dataset was extracted from the commit that implemented the
change, our partitioning is inclusive of that commit.  That is, we update the
model with the linked commit and infer the snapshot index from that commit.
This allows our evaluations to capture any entities added to address the issue
report, as well as changed entities, but does not capture any entities that were
removed by the change.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Setting}

Our document extraction process is shown on the left side of
Figure~\ref{fig:changeset}.  We implemented our document extractor in Python
v2.7 using the Dulwich
library\footnote{\url{http://www.samba.org/~jelmer/dulwich/}} for interacting
with the source code repository and
Teaser\footnote{\url{https://github.com/nkraft/teaser}} for parsing source code.
We extract documents from both a snapshot of the repository at a tagged snapshot
and each commit reachable from that tag's commit.  The same preprocessing steps
are employed on all extracted documents.

For our document extraction from a snapshot, we first parse each Java file using
our tool, Teaser, which is a text extractor implemented in Java using an open
source Java 1.5 grammar and ANTLR v3.  The tool extracts documents from the
chosen source code entity type, either methods or classes.  We consider
interfaces, enumerations, and annotation types to also be a class.  The text of
inner an entity (e.g., a method inside an anonymous class) is only attributed to
that entity, and not the containing one.  Comments, literals, and identifiers
within a entity are considered as text of the entity.  Block comments
immediately preceding an entity are also included in this text.

To extract text from the changesets, we look at the \texttt{git diff} between
two commits.  In our changeset text extractor, we extract all text related to
the change, e.g., context, removed, and added lines; metadata lines are ignored.
Note that we do not consider where the text originates from, only that it is
text changed by the commit.

After extracting tokens, we split the tokens based on camel case, underscores,
and non-letters.  We only keep the split tokens; original tokens are discarded.
We normalize to lower case before filtering non-letters, English stop
words~\cite{Fox_1992}, Java keywords, and words shorter than three characters
long.  We do not stem words.

We implemented our modeling using the Python library
Gensim~\cite{Rehurek-Sojk_2010}, version 0.10.3. We use the same configurations
on each subject system.  We do not try to adjust parameters between the
different systems to attempt to find a better, or best, solution; rather, we
leave them the same to reduce confounding variables.  We do realize that this
may lead to topic models that may not be best-suited for feature location on a
particular subject system.  However, this constraint gives us confidence that
the measurements collected are fair and that the results are not influenced by
selective parameter tweaking.  Again, our goal is to show the performance of the
changeset-based FLT against snapshot-based FLT under the same conditions.

Gensim's LDA implementation is based on an online LDA by Hoffman et
al.~\cite{Hoffman-etal_2010} and uses variational inference instead of a
collapsed Gibbs sampler.  Unlike Gibbs sampling, in order to ensure that the
model converges for each document, we allow LDA to see each mini-batch $5$ times
by setting Gensim's initialization parameter \texttt{passes} to this value and
allowing the inference step $1000$ iterations over a document.  We set the
following LDA parameters for all 6 systems: $500$ topics, a symmetric
$\alpha=1/K$, and a symmetric $\eta=1/K$.  These are default values for $\alpha$
and $\eta$ in Gensim, and have been found to work well for the FLT
task~\cite{Biggers-etal_2014}.

For the historical simulation, we found it beneficial to consider two other
parameters: $\kappa$ and $\tau_0$.  As noted in Hoffman et
al.~\cite{Hoffman-etal_2010}, it is beneficial to adjust $\kappa$ and $\tau_0$
to higher values for smaller mini-batches.  These two parameters control how
much influence a new mini-batch has on the model when training.  We follow the
recommendations in Hoffman et al.\, choosing $\tau_0=1024$ and $\kappa=0.9$ for
all systems, because the historical simulation often has mini-batch sizes in
single digits.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Data Collection and Analysis}
\label{sec:data}

To evaluate the performance of a TM-based FLT we cannot use measures such as
precision and recall.  This is because the FLT creates the rankings pairwise,
causing every entity being searched to appear in the rankings.  Poshyvanyk et
al. define an effectiveness measure that can be used for TM-based
FLTs~\cite{Poshyvanyk-etal_2007}.  The effectiveness measure is the rank of the
first relevant document and represents the number of source code entities a
developer would have to view before reaching a relevant one.  The effectiveness
measure allows evaluating the FLT by using the mean reciprocal rank
(MRR)~\cite{Voorhees_1999}:
\begin{equation}
    MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{e_i}
\end{equation}
where $Q$ is the set of queries and $e_i$ is the effectiveness measure for some
query $Q_i$.

To answer RQ1, we run the experiment on the snapshot and changeset corpora as
outlined in Section~\ref{sec:methodology}.  We then calculate the MRR between
the two sets of effectiveness measures.  We use the Wilcoxon signed-rank test
with Holm correction to determine the statistical significance of the difference
between the two rankings.  To answer RQ2, we run the historical simulation as
outlined in Section~\ref{sec:methodology} and compare it to the results of batch
changesets from RQ1.  Again, we calculate the MRR and use the Wilcoxon
signed-rank test.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Results}

\input{tables/rq1_lda}

RQ1 asks how well a topic model trained on changesets performs against one
trained on source code entities.  Table~\ref{table:rq1:class:lda} and
Table~\ref{table:rq1:method:lda} summarize the results of each subject system
when evaluated at the class and method granularity, respectively.  In each of
the tables, we bold which of the two MRRs is greater.  Since our goal is to show
that training with changesets is just as good, or better than, training on
snapshots, we only care about statistical significance when the MRR is in favor
of snapshots.

For LDA at the class-level we note an improvement in MRR for 2 of the 6 systems
when using changesets.  Additionally, 1 of these 2 systems were statistically
significant at $p<0.01$.  Only 1 of the 4 systems with MRR in favor of snapshots
were statistically significant.  Hence, changeset topics perform just as well as
snapshot topics at the class-level 5 of the 6 times.

For LDA at the method-level we note an improvement in MRR for 4 of the 6 systems
when using changesets.  None of these were statistically significant at
$p<0.01$.  This suggests that changeset topics are as accurate as snapshot
topics at the method-level, especially since there is a lack of statistical
significance for \emph{any} of the cases.

\begin{framed}
    \textbf{RQ1}:
    Changeset-based FLTs are as accurate as snapshot-based FLTs.
\end{framed}

\input{tables/rq2_lda}

RQ2 asks how well a simulation of using a topic model would perform as it were
to be used in real-time.  This is a much closer evaluation of an FLT to it being
used in an actual development environment.  Table~\ref{table:rq2:class:lda} and
Table~\ref{table:rq2:method:lda} summarize the results of each subject system
when evaluated at the class and method granularity, respectively.  In each of
the tables, we bold which of the two MRRs is greater.  Again, since our goal is
to show that temporal considerations must be given during FLT evaluation, we
only care about statistical significance when the MRR is in favor of batch.

At the class-level we note an improvement in MRR for 5 of the 6 systems with 4
of these 5 statistically significant at $p<0.01$.  The one result in favor of
batch changesets, for ArgoUML v0.24, was not statistically significant.  A the
method-level we note an improvement in MRR for 4 of the 6 systems with 2 of the
4 statistically significant at $p<0.01$.

\begin{framed}
    \textbf{RQ2}:
    Historical simulation reveals that the accuracy of the changeset-based FLT
    is consistent as a project evolves and is actually higher than indicated by
    batch evaluation.
\end{framed}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}

The results outlined in the previous section warrants some qualitative
discussion.  In particular, our analysis shows significant affects between
snapshots and changesets, and between batch changesets and changesets in the
simulated environment.  The results are mixed between each and are not
conclusive.  However, we argue this is desirable to show that the accuracy of a
changeset-based FLT is similar to that of a snapshot-based FLT but without the
retraining cost.

\begin{figure*}[t]
\centering
\includegraphics[width=0.36\textwidth]{figures/rq2-overall-class}
\includegraphics[width=0.36\textwidth]{figures/rq2-overall-method}
\caption{Effectiveness measures for classes (left) and methods (right) across all 6 systems}
\label{fig:em}
\end{figure*}

\subsection{RQ1}

Figure~\ref{fig:em} shows the effectiveness measures for methods and classes
across all systems. The figure suggests that snapshot-based models and
changeset-based models have similar results overall with changesets performing
slightly better, but does not help to understand how each feature query performs
for each model.  With respect to RQ1, we will investigate the queries and
effectiveness measures between the batch snapshot and batch changesets in
detail.

For the 632 successful queries of classes, each query returns the same
effectiveness measure 28 out of 632 times, or about 4.4\% of the time.  Of these
28, 17 of them all return an effectiveness measure of 1 (the best possible
measure).  For 159 queries (25.2\%), the effectiveness measure is within 10
ranks of each other.  For 291 queries (46.0\%), the effectiveness measure is
within 50 ranks of each other.  The remaining 341 queries (53.9\%) perform
noticeably different ($> 50$ ranks apart).

For the 629 successful queries of methods, each query returns the same
effectiveness measure 12 out of 629 times, or about 1.9\% of the time.  Of these
12, 7 return an effectiveness measure of 1 (the best possible measure).  For 65
queries (10.3\%), the effectiveness measure is within 10 ranks of each other.
For 151 queries (24.0\%), the effectiveness measure is within 50 ranks of each
other.  The remaining 478 queries (75.9\%) perform noticeably different ($> 50$
ranks apart).

\subsection{RQ2}

Figure~\ref{fig:em} also shows the effectiveness measures for methods and
classes across the 6 systems considered in RQ2. The figure shows that the
historical simulation outperforms both batch evaluations, but does not help to
understand how each feature query performs for each model.  With respect to RQ2,
we will investigate the queries and effectiveness measures between the
historical simulation and the batch evaluations in detail.

For the 603 successful queries of classes, each query returns the same
effectiveness measure 8 out of 603 times, or about 1.3\% of the time.  Of these
8, 7 return an effectiveness measure of 1 (the best possible measure).  For 111
queries (18.4\%), the effectiveness measure is within 10 ranks of each other.
For 230 queries (38.1\%), the effectiveness measure is within 50 ranks of each
other.  The remaining 373 queries (61.8\%) perform noticeably different ($> 50$
ranks apart).

For the 595 successful queries of methods, each query returns the same
effectiveness measure 3 out of 595 times, or about 0.5\% of the time.  Of these
3, all return an effectiveness measure of 1 (the best possible measure).  For 23
queries (3.9\%), the effectiveness measure is within 10 ranks of each other.
For 77 queries (12.9\%), the effectiveness measure is within 50 ranks of each
other.  The remaining 518 queries (87.0\%) perform noticeably different ($> 50$
ranks apart).

\subsection{Situations}

%%%% Awkwardddddddddd

In this study, we've also asked two research questions which lead to two
distinct comparisons.  First, we compare a batch TM-based FLT trained on the
changesets of a project's history to one trained on the snapshot of source code
entities.  Second, we compare a batch TM-based FLT trained on changesets to a
online TM-based FLT trained on the same changesets over time.  Our results are
mixed between the research questions, hence we end up with four possible
situations; we will now discuss each of these situations in detail.

\subsubsection{Batch changesets are better than batch snapshot
\emph{and} batch changesets are better than changesets in the simulated environment}

% Snapshots < Changeset && Batch > Temporal
% Class
%   ArgoUML 0.24
%
% Method
%   ArgoUML 0.22

This situation occurs in 1 out of 6 systems at the class-level, and 1 out of 6
systems at the method-level.  We hypothesize that this is due to the nature of
the batch evaluation versus the historical simulation.  In the batch evaluation,
the model is trained on all data before being queried, while in the historical
simulation the model is trained on partial data before being queried.  This
allows for the batch model to be more accurate because it is trained on more
data and reveals feature location research evaluations may not be accurately
portraying how an FLT would perform in a real scenario.

\subsubsection{Batch changesets are better than batch snapshot
\emph{and} changesets in the simulated environment are better than batch changesets}

% Snapshots < Changeset && Batch < Temporal
% Class
%   ArgoUML 0.22
%
% Method
%   JabRef
%   jEdit
%   muCommander

This situation occurs in 1 out of 6 systems at the class-level, and 3 out of 6
systems at the method-level.  We hypothesize that this is due to the same
previous reason, but that historical simulation more accurately captures the
correct state of the system (i.e., the source code entities) at the point in
time when querying is done.  Since querying on the batch models is after the
model is completely trained, there may be source code entities that do not exist
in the system anymore that were at one time changed to complete a certain task.
Again, the historical simulation better captures this scenario.

\subsubsection{Batch snapshot are better than batch changeset
\emph{and} changesets in the simulated environment are better than batch changesets}

%  Snapshots > Changeset && Batch < Temporal
% Class
%   ArgoUML 0.26
%   JabRef
%   jEdit
%   muCommander
%
% Method
%   ArgoUML 0.24
%   ArgoUML 0.26

This situation occurs in 4 out of 6 systems at the class-level, and 2 out of 6
systems at the method-level.  Similarly, this could be because of how the models
are trained.  Although the batch changesets performed worse in both cases, it
does improve during historical simulation.  This does not mean that changesets
are bad, but more accurately model the system over time.

\subsubsection{Batch snapshot are better than batch changeset
\emph{and} batch changesets are better than changesets in the simulated environment}

% Snapshots > Changeset && Batch > Temporal
% Class
%
% Method
%
We note that this situation never occurs.  This also supports the hypothesis
that historical simulation more accurately portrays the system over time.
However, we cannot conclude this without also historically simulating snapshot
TM-based FLTs.
