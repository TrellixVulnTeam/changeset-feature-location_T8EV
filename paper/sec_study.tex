% vim:syntax=tex

In this section we describe the design of a case study in which we
compare topic models trained on changesets to those trained on snapshots.
We describe the case study using the Goal-Question-Metric approach~\cite{Basili-etal:94}.
The data and source code for the case study is available in this paper's online
appendix\footnote{\textbf{REVIEWER URL ONLY:} \url{http://cscorley.students.cs.ua.edu/cfl/}}.

\subsection{Definition and Context}

% TODO
Our \textit{goal} is to evaluate the usefulness of topic models built
from changesets.
The \textit{quality focus} of the study is on informing development
decisions and policy changes that could lead to software with fewer
defects.
The \textit{perspective} of the study is of a researcher, developer, or
project manager who wishes to gain understanding of the concepts or
features implemented in the source code.
The \textit{context} of the study spans the version histories of 14
open source systems.

Toward achievement of our goal, we pose the following research questions:
\begin{description}[font=\itshape\mdseries,leftmargin=10mm,style=sameline]
% TODO
    \item[RQ1] How well do changeset-based topic models perform for feature location?
    \item[RQ2] How well do \emph{temporal simulations} of changeset-based topic models perform for feature location?
\end{description}
At a high level, we want to determine the feasibility in using changesets
to train topic models for feature location.

In the remainder of this section we introduce the subjects of our study,
describe the setting of our study, and report our data collection and analysis procedures.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Subject software systems}

All of our subject software systems come from two publicly-available
datasets.  The first is a dataset of four software systems by Dit et
al.~\cite{Dit:2013} and contains method-level goldsets.  The second is
a dataset of 14 software systems by Moreno et
al.~\cite{Moreno:2014} and contains class-level goldsets. The four
software systems in the first dataset also appear in the second,
supplying us with both class- and method-level goldsets for the queries.

\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\footnotesize
\centering
\caption{Subject Systems and Goldset Sizes}
\begin{tabular}{lrrr}
    \toprule
    Subject System      & Features  & Classes   & Methods \\
    \midrule
    ArgoUML v0.22 & 91 & 287 & 701 \\
    ArgoUML v0.24 & 52 & 154 & 357 \\
    ArgoUML v0.26.2 & 209 & 706 & 1560 \\
    BookKeeper v4.1.0 & 40 & 152 &   \\
    Derby v10.7.1.1 & 32 & 55 &   \\
    Derby v10.9.1.0 & 95 & 410 &   \\
    Hibernate v3.5.0b2 & 20 & 53 &   \\
    Jabref v2.6 & 39 & 131 & 280 \\
    jEdit v4.3 & 150 & 361 & 748 \\
    Lucene v4.0 & 35 & 103 &   \\
    Mahout v0.8 & 30 & 159 &   \\
    muCommander v0.8.5 & 92 & 303 & 717 \\
    OpenJPA v2.0.1 & 35 & 82 &   \\
    OpenJPA v2.2.0 & 18 & 53 &   \\
    Pig v0.8.0 & 85 & 442 &   \\
    Pig v0.11.1 & 48 & 129 &   \\
    Solr v4.4.0 & 55 & 189 &   \\
    Tika v1.3 & 18 & 34 &   \\
    ZooKeeper v3.4.5 & 80 & 285 &   \\
    \midrule
    Total & 1224 & 4088 & 4363 \\
    \bottomrule
\end{tabular}
\label{table:subjects}
\end{table}

ArgoUML is a UML CASE tool that supports standard UML diagrams\footnote{\url{http://argouml.tigris.org/}}.
BookKeeper is a distributed logging service\footnote{\url{http://zookeeper.apache.org/bookkeeper/}}.
Derby is a relational database management system\footnote{\url{http://db.apache.org/derby/}}.
Eclipse is an intergrated development environment to develop applications in various programming languages\footnote{\url{https://www.eclipse.org/}}.
Hibernate is a java package used to work with relational databases\footnote{\url{http://hibernate.org/}}.
JEdit is a Java text editor\footnote{\url{http://www.jedit.org/}}.
JabRef is a tool for managing bibliographical reference data\footnote{\url{http://http://jabref.sourceforge.net/}}.
Lucene is an information retrieval library written in Java\footnote{\url{http://lucene.apache.org/core/}}.
Mahout is a tool for scaleable machine learning\footnote{\url{https://mahout.apache.org/}}.
MuCommander is a cross-platform file manager\footnote{\url{http://www.mucommander.com/}}.
OpenJPA is object relational mapping tool\footnote{\url{http://openjpa.apache.org/}}.
Pig is a platform for analyzing large datasets consisting of high-level language\footnote{\url{http://pig.apache.org/}}.
Solr is an enterprised search platform\footnote{\url{http://lucene.apache.org/solr/}}.
Tika is a toolkit for extracting metadata and text from various types of files\footnote{\url{http://tika.apache.org/}}.
ZooKeeper is a tool that works as a coordination service to help build distributed applications\footnote{\url{http://zookeeper.apache.org/bookkeeper/}}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Methodology}
\label{sec:methodology}

For snapshots, the process is straightforward.
First, we build a model in batch mode from the snapshot corpus.
That is, the model can see all documents in the corpus at once.
Then, we infer a $\theta_{snapshot}$ from the snapshot corpus
and a $\theta_{queries}$ from the query corpus.
Finally, we classifiy the results from both $\theta$s.

\begin{comment}
\begin{enumerate}
    \item Build model from the snapshot corpus in batch mode
    \item Infer a $\theta_{snapshot}$ from the snapshot corpus
    \item Infer a $\theta_{queries}$ from the query corpus
    \item Classifiy, or rank, the results from both $\theta$s
\end{enumerate}
\end{comment}


For changesets, the process is varies slightly from a snapshot approach.
First, we build a model in batch mode from the changeset corpus.
Then, we infer a $\theta_{snapshot}$ from the snapshot corpus
and a $\theta_{queries}$ from the query corpus.
Note that we \emph{do not} infer a $\theta_{changesets}$ from the changeset corpus from which the model was built!
Finally, we classifiy the results from both $\theta$s.

\begin{comment}
\begin{enumerate}
    \item Build model from the changeset corpus in batch mode
    \item \emph{Do not} infer a $\theta_{changesets}$
    \item Infer a $\theta_{snapshot}$ from the snapshot corpus
    \item Infer a  $\theta_{queries}$ from the query corpus
    \item Classifiy, or rank, the results from both $\theta$s
\end{enumerate}
\end{comment}


For the temporal simulation, we can take a slightly different approach.
We first determine which changesets relate to an issue and partition mini-batches out of the changesets.
Then, we initialize a model in online mode.
For each mini-batch, or partition, we update the model with that mini-batch.
Then, we infer a $\theta_{snapshot}$ from the snapshot corpus
and a retrieve the $\theta_{queries}$ for the queries related to this changeset.
Finally, we classifiy the results from both $\theta$s.

\begin{comment}
\begin{enumerate}
    \item Initialize a model in online mode
    \item Determine which changesets relate to an issue and partition mini-batches out of the changesets
    \item For each mini-batch:
        \begin{enumerate}
            \item Update the model with mini-batch
            \item Update $\theta_{snapshot}$ with the new inference of the source code document affected by this changeset
            \item Infer a $\theta_{query}$ of the query related to the changeset we stopped at
            \item Classifiy, or rank, the results from both $\theta$s
        \end{enumerate}
\end{enumerate}
\end{comment}

Since the Dit et al. dataset was extracted from the commit that implemented the change,
we our partitioning is inclusive of that commit.
That is, we update the model with the linked commit and infer the
$\theta_{Snapshot}$ from that commit.
This allows our evaluation to capture any entities added to address the issue report,
as well as changed entities,
but does not capture any entities that were removed by the change.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Setting}

Our document extraction process is shown on the left side of Figure~\ref{fig:changeset}
We implemented our document extractor in Python v2.7
using the Dulwich library\footnote{\url{http://www.samba.org/~jelmer/dulwich/}}
for interacting with the source code repository and
Taser\footnote{\url{http://xxxxxxxxxxx}} for parsing source code.
We extract documents from both a snapshot of the repository at a tagged
snapshot and each commit reachable from that tag's commit.
The same preprocessing steps are employed on all documents extracted.

For our document extraction from a snapshot, we first parse each Java file using our tool, Taser.
Taser is a text extractor implemented in Java using an open source Java 1.5 grammar and ANTLR v3.
The tool extracts documents from the chosen source code entity type,
either methods or classes, and treats inner entities to be distinct from the outer entity.
We consider interfaces, enumerations, and annotation types to also be a class.
The text of inner an entity (e.g., a method inside an anonymous class)
is only attributed to that entity, and not the containing one.
Comments, literals, and identifiers within a entity are considered as text of the entity.
Block comments immediately preceding an entity are also included in this text.

To extract text from the changesets, we look at the output of viewing
the \texttt{git diff} between two commits.
In our changeset text extractor, we extract all text related to the
change, e.g., context, removed, and added lines; metadata lines are ignored.
Note that we do not consider where the text originates from,
only that it is text changed by the commit.\footnote{
The Apache Lucene and Solr projects were merged into a single, new repository
during their development.
We only use changes that affect each project's subfolder in the merged repository,
and also include all changes from the two pre-merge repositories in each project's respective corpus.
}

After extracting tokens, we split the tokens based on camel case,
underscores, and non-letters.
We only keep the split tokens; original tokens are discarded.
We normalize to lower case before filtering non-letters, English stop words~\cite{StopWords}, Java keywords, and words shorter than three characters long.
We do not stem words. \attn{cite reason?}

We implemented our modeling using the Python library Gensim~\cite{Gensim}.
Our evaluation is two-fold: we compare results for both LDA and LSI.
We do this to evaluate the performance of using changesets as input
documents regardless of chosen algorithm.

\subsubsection{LDA configuration}

Gensim's LDA implementation is based on an online LDA by Hoffman et al.~\cite{Hoffman-etal:2010}
and uses variational inference instead of a Collapsed Gibbs Sampler.
Unlike Gibbs sampling, in order to ensure that the model converges for each document,
we allow LDA to see each mini-batch $5$ times by setting Gensim's initialization parameter \texttt{passes} to this value
and allowing the inference step $1000$ iterations over a document.
We set the following LDA parameters for all 14 systems:
$500$ topics ($K$), a symmetric $\alpha=1/K$, and a symmetric $\beta=1/K$.
These are default values for $\alpha$ and $\beta$ in Gensim.

For the temporal evaluation, we found it beneficial to consider two other parameters: $\kappa$ and $\tau_0$.
As noted in Hoffman et al.~\cite{Hoffman-etal:2010}, it is beneficial to
adjust $\kappa$ and $\tau_0$ to higher values for smaller mini-batches.
These two parameters control how much influence a new mini-batch has on the model when training.
However, the implementation of Gensim at the time of this paper did not
include a way to adjust $\tau_0$, so we only adjust $\kappa$.
We chose $\kappa=2.0$ for all systems, because the temporal evaluation
would often have mini-batch sizes in single digits.

\subsubsection{LSI configuration}

% TODO
Gensim's LSI implementation is based on an online LSI by {\v R}eh{\r u}{\v r}ek\cite{Radim:2011}.
We set the number of topics the same as LDA for each project.
The remaining parameters are left at default values, including those used
during the temporal evaluation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Data Collection and Analysis}
\label{sec:data}

\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\footnotesize
\centering
\caption{Reduced Temporal Goldset Sizes}
\begin{tabular}{lrrr}
    \toprule
    Subject System      & Features  & Classes   & Methods \\
    \midrule
    ArgoUML v0.22 & 91 & 287 & 701 \\
    ArgoUML v0.24 & 52 & 154 & 357 \\
    ArgoUML v0.26.2 & 191 & 627 & 1400 \\
    BookKeeper v4.1.0 & 12 & 44 &   \\
    Derby v10.7.1.1 & 4 & 7 &   \\
    Derby v10.9.1.0 & 12 & 40 &   \\
    Jabref v2.6 & 30 & 113 & 254 \\
    jEdit v4.3 & 148 & 359 & 746 \\
    Mahout v0.8 & 2 & 42 &   \\
    muCommander v0.8.5 & 91 & 302 & 716 \\
    OpenJPA v2.0.1 & 12 & 33 &   \\
    OpenJPA v2.2.0 & 5 & 29 &   \\
    Solr v4.4.0 & 20 & 97 &   \\
    Tika v1.3 & 4 & 11 &   \\
    ZooKeeper v3.4.5 & 21 & 89 &   \\
    \midrule
Total & 695 & 2234 & 4174
\end{tabular}
\label{table:subjects:temporal}
\end{table}

To evaluate the performance of a topic-modeling-based FLT we cannot use
measures such as precision and recall. This is because the FLT creates
the rankings pairwise, causing every entity being searched to appear in the rankings.
Poshyvanky et al. define an effectiveness measure that can be used for topic-modeling-based FLTs~\cite{Poshyvanyk-etal:2007}.
The effectiveness measure is the rank of the first relevant document
and represents the number of source code entities a developer would have to view before reaching a relevant one.
The effectiveness measure allows evaluating the FLT by using
the mean reciprocal rank (MRR)~\cite{Voorhees:1999}, defined as:

\begin{equation}
    MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{e_i}
\end{equation}

where $Q$ is the set of queries
and $e_i$ is the effectiveness measure for some query $Q_i$.

To answer RQ1, we run the experiment on the snapshot and changeset
datasets as outlined in Section~\ref{sec:methodology}.
We then calculate the MRR between the two.

To answer RQ2, we run the experiment temporally as outlined in Section~\ref{sec:methodology}
and compare it to the results from the snapshot experiment.
We then calculate the MRR between the two.
However, only the Dit et al. dataset includes traceability links between
the queries and the commits the goldsets are extracted from.
We did a best-effort search for the Moreno et al. traceability links
using regular expressions.
We were able to recover portions of the links for all subject systems in
the Moreno et al. dataset with the exception of Hibernate, Pig (both versions), and Lucene.
The issues in these subject systems could not be found by searching the commit logs.
Additionally, in some cases, only a partial amount of the links could be
recovered. In light of this, we reduced the evaluation for comparing the
snapshot and temporal results to only those features found.
An updated version of Table~\ref{table:subjects} can be found in Table~\ref{table:subjects:temporal}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Results}



% TODO
RQ1 asks how well a topic model built upon changesets performs against
one built on source code entities.
We also wanted to know if there were any differences of performance
between two popular algorithms, LSI and LDA.
The left sides of Table~\ref{table:rq1:classes} and Table~\ref{table:rq1:methods}
summarize the results for each system with regard to RQ1.

\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\centering
\caption{RQ1: MRR of subject systems classes}
\begin{tabular}{l|ll|ll}
    \toprule
                        & \multicolumn{2}{c|}{LDA}      &  \multicolumn{2}{c}{LSI}  \\
    Subject System      & Snapshot      & Changeset     & Snapshot      & Changeset   \\
    \midrule
ArgoUML v0.22 & 0.117894 & {\bf 0.147477 } & 0.023883 & {\bf 0.025465 }  \\
ArgoUML v0.24 & {\bf 0.185603 } & 0.136062 & 0.001501 & {\bf 0.002817 }  \\
ArgoUML v0.26.2 & {\bf 0.226795 } & 0.152747 & 0.002669 & {\bf 0.006168 }  \\
Bookkeeper v4.1.0 & 0.017075 & {\bf 0.041524 } & 0.003611 & {\bf 0.011522 }  \\
Derby v10.7.1.1 & {\bf 0.016232 } & 0.003174 & {\bf 0.000659 } & 0.000543  \\
Derby v10.9.1.0 & 0.003839 & {\bf 0.010012 } & 0.000552 & {\bf 0.000592 }  \\
Hibernate v3.5.0b2 & {\bf 0.003333 } & 0.000869 & 0.000537 & {\bf 0.001483 }  \\
Jabref v2.6 & {\bf 0.282994 } & 0.151127 & 0.001187 & {\bf 0.001195 }  \\
jEdit v4.3 & {\bf 0.280224 } & 0.134725 & {\bf 0.012334 } & 0.009393  \\
Lucene v4.0 & 0.001180 & {\bf 0.001233 } & {\bf 0.001145 } & 0.000408  \\
Mahout v0.8 & 0.002976 & {\bf 0.003310 } & 0.001032 & {\bf 0.001165 }  \\
muCommander v0.8.5 & {\bf 0.268472 } & 0.211183 & 0.003516 & {\bf 0.013408 }  \\
OpenJPA v2.0.1 & 0.001713 & {\bf 0.002066 } & {\bf 0.000368 } & 0.000342  \\
OpenJPA v2.2.0 & 0.001635 & {\bf 0.003255 } & {\bf 0.001121 } & 0.000549  \\
Pig v0.8.0 & 0.007438 & {\bf 0.015208 } & 0.001292 & {\bf 0.001572 }  \\
Pig v0.11.1 & {\bf 0.027005 } & 0.009604 & 0.000988 & {\bf 0.011666 }  \\
Solr v4.4.0 & {\bf 0.002795 } & 0.002150 & {\bf 0.000570 } & 0.000526 \\
Tika v1.3 & 0.007832 & {\bf 0.008343 } & 0.002084 & {\bf 0.002149 }  \\
Zookeeper v3.4.5 & 0.023125 & {\bf 0.025065 } & {\bf 0.003748 } & 0.003280  \\
    \midrule
All & {\bf 0.124513 } & 0.086323 & 0.004832 & {\bf 0.006649 } \\
    \bottomrule
\end{tabular}
\label{table:rq1:classes}
\end{table}


RQ2 asks how well a simulation of using a topic model would perform as
it were to be used in real-time.
This is a much closer evaluation of a feature location technique to it
being used in an actual development environment.
We also wanted to know if there were any differences of performance
between two popular algorithms, LSI and LDA.
The right sides of Table~\ref{table:rq2:classes} and Table~\ref{table:rq2:methods}
summarize the results for each system with regard to RQ2.


\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\centering
\caption{RQ1: MRR of subject systems methods}
\begin{tabular}{l|ll|ll}
    \toprule
                         & \multicolumn{2}{c|}{LDA}      &  \multicolumn{2}{c}{LSI}  \\
    Subject System       & Snapshot      & Temporal      & Snapshot      & Temporal  \\
    \midrule
ArgoUML v0.22 & 0.044457 & {\bf 0.073552 } & {\bf 0.055535 } & 0.047246  \\
ArgoUML v0.24 & 0.057022 & {\bf 0.086003 } & {\bf 0.052374 } & 0.050937  \\
ArgoUML v0.26.2 & {\bf 0.075917 } & 0.067152 & {\bf 0.067679 } & 0.056058  \\
Jabref v2.6 & {\bf 0.086691 } & 0.036863 & {\bf 0.005267 } & 0.003559  \\
jEdit v4.3 & {\bf 0.065631 } & 0.049099 & 0.038576 & {\bf 0.045326 }  \\
muCommander v0.8.5 & 0.037569 & {\bf 0.046009 } & 0.021294 & {\bf 0.022017 }  \\
    \midrule
All & {\bf 0.062533 } & 0.060422 & {\bf 0.047253 } & 0.043663  \\
    \bottomrule
\end{tabular}
\label{table:rq1:methods}
\end{table}

\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\centering
\caption{RQ2: MRR of subject systems classes}
\begin{tabular}{l|ll|ll}
    \toprule
                        & \multicolumn{2}{c|}{LDA}      &  \multicolumn{2}{c}{LSI}  \\
    Subject System      & Snapshot      & Temporal      & Snapshot      & Temporal  \\
    \midrule
ArgoUML v0.22 & {\bf 0.117894 } & 0.111940 & 0.023883 & {\bf 0.056540 } \\
ArgoUML v0.24 & {\bf 0.185603 } & 0.162652 & 0.001501 & {\bf 0.100166 } \\
ArgoUML v0.26.2 & {\bf 0.235158 } & 0.205164 & 0.001235 & {\bf 0.049686 } \\
Bookkeeper v4.1.0 & {\bf 0.037099 } & 0.006553 & 0.006395 & {\bf 0.027403 } \\
Derby v10.7.1.1 & {\bf 0.050524 } & 0.007924 & 0.000526 & {\bf 0.005237 } \\
Derby v10.9.1.0 & 0.001168 & {\bf 0.007514 } & 0.000474 & {\bf 0.002600 } \\
Jabref v2.6 & {\bf 0.298368 } & 0.235677 & 0.001226 & {\bf 0.099428 } \\
jEdit v4.3 & 0.281964 & {\bf 0.283543 } & 0.012411 & {\bf 0.035548 } \\
Mahout v0.8 & {\bf 0.009379 } & 0.001935 & 0.001378 & {\bf 0.002907 } \\
muCommander v0.8.5 & 0.271410 & {\bf 0.282706 } & 0.003548 & {\bf 0.117789 } \\
OpenJPA v2.0.1 & 0.002413 & {\bf 0.005554 } & 0.000551 & {\bf 0.003109 } \\
OpenJPA v2.2.0 & 0.000504 & {\bf 0.025957 } & 0.003423 & {\bf 0.003690 } \\
Solr v4.4.0 & 0.001809 & {\bf 0.003087 } & 0.000741 & {\bf 0.007636 } \\
Tika v1.3 & 0.013718 & {\bf 0.052773 } & 0.002140 & {\bf 0.056250 } \\
Zookeeper v3.4.5 & {\bf 0.038860 } & 0.007438 & 0.002208 & {\bf 0.019434 } \\
    \midrule
All & {\bf 0.204739 } & 0.191973 & 0.006999 & {\bf 0.057604 } \\
    \bottomrule
\end{tabular}
\label{table:rq2:classes}
\end{table}


\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\centering
\caption{RQ2: MRR of subject systems methods}
\begin{tabular}{l|ll|ll}
    \toprule
                        & \multicolumn{2}{c|}{LDA}      &  \multicolumn{2}{c}{LSI}  \\
    Subject System      & Snapshot      & Temporal      & Snapshot      & Temporal  \\
    \midrule
ArgoUML v0.22 & 0.045452 & {\bf 0.072741 } & {\bf 0.056780 } & 0.037277 \\
ArgoUML v0.24 & {\bf 0.055925 } & 0.033152 & {\bf 0.051367 } & 0.048172 \\
ArgoUML v0.26.2 & {\bf 0.076920 } & 0.073376 & {\bf 0.066626 } & 0.045590 \\
Jabref v2.6 & {\bf 0.107592 } & 0.068359 & 0.006727 & {\bf 0.049223 } \\
jEdit v4.3 & 0.065625 & {\bf 0.071610 } & {\bf 0.038575 } & 0.027338 \\
muCommander v0.8.5 & 0.038280 & {\bf 0.070755 } & 0.021766 & {\bf 0.042647 } \\
    \midrule
All & 0.063426 & {\bf 0.068713 } & {\bf 0.047242 } & 0.039814 \\
    \bottomrule
\end{tabular}
\label{table:rq2:methods}
\end{table}

