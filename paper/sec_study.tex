% vim:syntax=tex

In this section we describe the design of a case study in which we
compare topic models trained on changesets to those trained on snapshots.
We describe the case study using the Goal-Question-Metric approach~\cite{Basili-etal:94}.
The data and source code for the case study is available in this paper's online
appendix\footnote{\textbf{REVIEWER URL ONLY:} \url{http://cscorley.students.cs.ua.edu/cfl/}}.

\subsection{Definition and Context}

% TODO
Our \textit{goal} is to evaluate the effective nature of topic models built
from changesets.
The \textit{quality focus} of the study is on informing development
decisions and policy changes that could lead to software with fewer
defects.
The \textit{perspective} of the study is of a researcher, developer, or
project manager who wishes to gain understanding of the concepts or
features implemented in the source code.
The \textit{context} of the study spans the version histories of 14
open source systems.

Toward the achievement of our goal, we pose the following research questions:
\begin{description}[font=\itshape\mdseries,leftmargin=10mm,style=sameline]
% TODO
    \item[RQ1] How well do changeset-based topic models perform for feature location?
    \item[RQ2] How well do \emph{temporal simulations} of changeset-based topic models perform for feature location?
\end{description}
At a high level, we want to determine the feasibility in using changesets
to train topic models for feature location.

In the remainder of this section we introduce the subjects of our study,
describe the setting of our study, and report our data collection and analysis procedures.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Subject software systems}

All of our subject software systems come from two publicly-available
datasets.
The first is a dataset of four software systems by Dit et al.~\cite{Dit-etal:2013} and contains method-level goldsets.
This dataset was automatically extracted from changesets that relate to the queries (issue reports).
The second is a dataset of 14 software systems by Moreno et al.~\cite{Moreno-etal:2014} and contains class-level goldsets.
This dataset was automatically extracted from patches attached to issue reports.
The four software systems in the first dataset also appear in the second,
supplying us with both class- and method-level goldsets for the queries.

\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\footnotesize
\centering
\caption{Subject Systems and Goldset Sizes}
\begin{tabular}{lrrr}
    \toprule
    Subject System     & Features & Classes & Methods \\
    \midrule
    ArgoUML v0.22      & 91       & 287     & 701     \\
    ArgoUML v0.24      & 52       & 154     & 357     \\
    ArgoUML v0.26.2    & 209      & 706     & 1560    \\
    BookKeeper v4.1.0  & 40       & 152     &         \\
    Derby v10.7.1.1    & 32       & 55      &         \\
    Derby v10.9.1.0    & 95       & 410     &         \\
    Hibernate v3.5.0b2 & 20       & 53      &         \\
    Jabref v2.6        & 39       & 131     & 280     \\
    jEdit v4.3         & 150      & 361     & 748     \\
    Lucene v4.0        & 35       & 103     &         \\
    Mahout v0.8        & 30       & 159     &         \\
    muCommander v0.8.5 & 92       & 303     & 717     \\
    OpenJPA v2.0.1     & 35       & 82      &         \\
    OpenJPA v2.2.0     & 18       & 53      &         \\
    Pig v0.8.0         & 85       & 442     &         \\
    Pig v0.11.1        & 48       & 129     &         \\
    Solr v4.4.0        & 55       & 189     &         \\
    Tika v1.3          & 18       & 34      &         \\
    ZooKeeper v3.4.5   & 80       & 285     &         \\
    \midrule
    Total              & 1224     & 4088    & 4363    \\
    \bottomrule
\end{tabular}
\label{table:subjects}
\end{table}

ArgoUML is a UML CASE tool that supports standard UML diagrams\footnote{\url{http://argouml.tigris.org/}}.
BookKeeper is a distributed logging service\footnote{\url{http://zookeeper.apache.org/bookkeeper/}}.
Derby is a relational database management system\footnote{\url{http://db.apache.org/derby/}}.
Eclipse is an intergrated development environment to develop applications in various programming languages\footnote{\url{https://www.eclipse.org/}}.
Hibernate is a java package used to work with relational databases\footnote{\url{http://hibernate.org/}}.
JEdit is a Java text editor\footnote{\url{http://www.jedit.org/}}.
JabRef is a tool for managing bibliographical reference data\footnote{\url{http://jabref.sourceforge.net/}}.
Lucene is an information retrieval library written in Java\footnote{\url{http://lucene.apache.org/core/}}.
Mahout is a tool for scaleable machine learning\footnote{\url{https://mahout.apache.org/}}.
MuCommander is a cross-platform file manager\footnote{\url{http://www.mucommander.com/}}.
OpenJPA is object relational mapping tool\footnote{\url{http://openjpa.apache.org/}}.
Pig is a platform for analyzing large datasets consisting of high-level language\footnote{\url{http://pig.apache.org/}}.
Solr is an enterprised search platform\footnote{\url{http://lucene.apache.org/solr/}}.
Tika is a toolkit for extracting metadata and text from various types of files\footnote{\url{http://tika.apache.org/}}.
ZooKeeper is a tool that works as a coordination service to help build distributed applications\footnote{\url{http://zookeeper.apache.org/bookkeeper/}}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Methodology}
\label{sec:methodology}

For snapshots, the process is straightforward.
First, we build a model in batch mode from the snapshot corpus.
That is, the model can see all documents in the corpus at once.
Then, we infer a $\theta_{Snapshot}$ from the snapshot corpus
and a $\theta_{Queries}$ from the query corpus.
Finally, we classify the results from both $\theta$s.

\begin{comment}
\begin{enumerate}
    \item Build model from the snapshot corpus in batch mode
    \item Infer a $\theta_{Snapshot}$ from the snapshot corpus
    \item Infer a $\theta_{Queries}$ from the query corpus
    \item Classify, or rank, the results from both $\theta$s
\end{enumerate}
\end{comment}


In terms of changesets, the process varies slightly from a snapshot approach.
First, we build a model in batch mode from the changeset corpus.
Second, we infer a $\theta_{Snapshot}$ from the snapshot corpus
and a $\theta_{Queries}$ from the query corpus.
Note that we \emph{do not} infer a $\theta_{Changesets}$ from the changeset corpus from which the model was built.
Finally, we classify the results from both $\theta$s.

\begin{comment}
\begin{enumerate}
    \item Build model from the changeset corpus in batch mode
    \item \emph{Do not} infer a $\theta_{Changesets}$
    \item Infer a $\theta_{Snapshot}$ from the snapshot corpus
    \item Infer a  $\theta_{Queries}$ from the query corpus
    \item Classify, or rank, the results from both $\theta$s
\end{enumerate}
\end{comment}


For the temporal simulation, we can take a slightly different approach.
We first determine which changesets relate to an issue and partition mini-batches out of the changesets.
We then proceed by initializing a model in online mode.
Using each mini-batch, or partition, we update the model.
Then, we infer a $\theta_{Snapshot}$ from the snapshot corpus
and a retrieve the $\theta_{Queries}$ for the queries related to this changeset.
Finally, we classify the results from both $\theta$s.

\begin{comment}
\begin{enumerate}
    \item Initialize a model in online mode
    \item Determine which changesets relate to an issue and partition mini-batches out of the changesets
    \item For each mini-batch:
        \begin{enumerate}
            \item Update the model with mini-batch
            \item Update $\theta_{Snapshot}$ with the new inference of the source code document affected by this changeset
            \item Infer a $\theta_{Query}$ of the query related to the changeset we stopped at
            \item Classify, or rank, the results from both $\theta$s
        \end{enumerate}
\end{enumerate}
\end{comment}

Since the Dit et al. dataset was extracted from the commit that implemented the change,
our partitioning is inclusive of that commit.
That is, we update the model with the linked commit and infer the
$\theta_{Snapshot}$ from that commit.
This allows our evaluation to capture any entities added to address the issue report,
as well as changed entities,
but does not capture any entities that were removed by the change.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Setting}

Our document extraction process is shown on the left side of Figure~\ref{fig:changeset}
We implemented our document extractor in Python v2.7
using the Dulwich library\footnote{\url{http://www.samba.org/~jelmer/dulwich/}}
for interacting with the source code repository and
Taser for parsing source code.
We extract documents from both a snapshot of the repository at a tagged
snapshot and each commit reachable from that tag's commit.
The same preprocessing steps are employed on all documents extracted.

For our document extraction from a snapshot, we first parse each Java file using our tool, Taser.
Taser is a text extractor implemented in Java using an open source Java 1.5 grammar and ANTLR v3.
The tool extracts documents from the chosen source code entity type,
either methods or classes, and treats inner entities to be distinct from the outer entity.
We consider interfaces, enumerations, and annotation types to also be a class.
The text of inner an entity (e.g., a method inside an anonymous class)
is only attributed to that entity, and not the containing one.
Comments, literals, and identifiers within a entity are considered as text of the entity.
Block comments immediately preceding an entity are also included in this text.

To extract text from the changesets, we look at
the \texttt{git diff} between two commits.
In our changeset text extractor, we extract all text related to the
change, e.g., context, removed, and added lines; metadata lines are ignored.
Note that we do not consider where the text originates from,
only that it is text changed by the commit.%\footnote{
%The Apache Lucene and Solr projects were merged into a single, new repository
%during their development.
%We only use changes that affect each project's subdirectory in the merged repository,
%and also include all changes from the two pre-merge repositories in each project's respective corpus.
%}

After extracting tokens, we split the tokens based on camel case,
underscores, and non-letters.
We only keep the split tokens; original tokens are discarded.
We normalize to lower case before filtering non-letters, English stop words~\cite{StopWords}, Java keywords, and words shorter than three characters long.
We do not stem words.

We implemented our modeling using the Python library Gensim~\cite{Gensim}, version 0.10.2.
Our evaluation is two-fold: we compare results for both LDA and LSI.
We do this to evaluate the performance of using changesets as input
documents regardless of chosen algorithm.

\subsubsection{LDA configuration}

Gensim's LDA implementation is based on an online LDA by Hoffman et al.~\cite{Hoffman-etal:2010}
and uses variational inference instead of a Collapsed Gibbs Sampler.
Unlike Gibbs sampling, in order to ensure that the model converges for each document,
we allow LDA to see each mini-batch $5$ times by setting Gensim's initialization parameter \texttt{passes} to this value
and allowing the inference step $1000$ iterations over a document.
We set the following LDA parameters for all 14 systems:
$500$ topics ($K$), a symmetric $\alpha=1/K$, and a symmetric $\beta=1/K$.
These are default values for $\alpha$ and $\beta$ in Gensim.

For the temporal evaluation, we found it beneficial to consider two other parameters: $\kappa$ and $\tau_0$.
As noted in Hoffman et al.~\cite{Hoffman-etal:2010}, it is beneficial to
adjust $\kappa$ and $\tau_0$ to higher values for smaller mini-batches.
These two parameters control how much influence a new mini-batch has on the model when training.
However, the implementation of Gensim at the time of this paper did not
include a way to adjust $\tau_0$, so we only adjust $\kappa$.
We chose $\kappa=2.0$ for all systems, because the temporal evaluation
often have mini-batch sizes in single digits.

\subsubsection{LSI configuration}

% TODO
Gensim's LSI implementation is based on an online LSI by {\v R}eh{\r u}{\v r}ek\cite{Radim:2011}.
We set the number of topics the same as LDA for each project.
The remaining parameters are left at default values, including those used
during the temporal evaluation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Data Collection and Analysis}
\label{sec:data}

To evaluate the performance of a topic-modeling-based FLT we cannot use
measures such as precision and recall. This is because the FLT creates
the rankings pairwise, causing every entity being searched to appear in the rankings.
Poshyvanyk et al. define an effectiveness measure that can be used for topic-modeling-based FLTs~\cite{Poshyvanyk-etal:2007}.
The effectiveness measure is the rank of the first relevant document
and represents the number of source code entities a developer would have to view before reaching a relevant one.
The effectiveness measure allows evaluating the FLT by using
the mean reciprocal rank (MRR)~\cite{Voorhees:1999}, defined as:

\begin{equation}
    MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{e_i}
\end{equation}

where $Q$ is the set of queries
and $e_i$ is the effectiveness measure for some query $Q_i$.

To answer RQ1, we run the experiment on the snapshot and changeset
datasets as outlined in Section~\ref{sec:methodology}.
We then calculate the MRR between the two.
We use the Wilcoxon signed-rank test with Holm correction to determine
the statistical significance of the difference between the two rankings.

To answer RQ2, we run the experiment temporally as outlined in Section~\ref{sec:methodology}
and compare it to the results of changesets from RQ1.
We then calculate the MRR between the two.
Again, we use the Wilcoxon signed-rank test.

However, only the Dit et al.\ dataset includes traceability links between
the queries and the commits the goldsets are extracted from.
With respect to RQ2, we do not report on the entire Moreno et al.\ dataset.
We do include the projects in common with the Dit et al.\ dataset
since these queries and goldsets are the same.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Results}

\input{tables/rq1_lda}

\input{tables/rq1_lsi}


RQ1 asks how well a topic model built upon changesets performs against
one built on source code entities.
We also wanted to know if there were any differences of performance
between two popular algorithms, LSI and LDA.
Table~\ref{table:rq1:class:lda} and Table~\ref{table:rq1:class:lsi}
summarize the results of each subject system when
evaluated at the class granularity with LDA and LSI, respectively.
Similarly, Table~\ref{table:rq1:method:lda} and Table~\ref{table:rq1:method:lsi}
summarize the results of each subject system when
evaluated at the method granularity.

For LDA at the class-level we note an improvement in MRR for 10 of the 19 systems when using changesets.
Additionally, 5 of these 10 systems were statistically significant at $p<0.01$.
Only 2 of the 9 systems with MRR in favor of snapshots were statistically significant.
Likewise, for the LSI evaluations at class granularity, 12 of the 19 systems improve with changesets.
However, none of the systems presented a statistically significant value.
Overall, for LDA, changesets perform slightly worse than snapshots with statistical significance.
LSI, again, performs slightly better with snapshots with no statistical significance.

For LDA at the method-level we note an improvement in MRR for 3 of the 6 systems when using changesets.
None of these were statistically significant at $p<0.01$.
Likewise, for the LSI evaluations at class granularity, 2 of the 6 systems improve with changesets
Again, none of the systems presented a statistically significant value.
Overall, for LDA, changesets perform slightly worse than snapshots with statistical significance.
LSI, again, performs slightly better with snapshots with no statistical significance.



\input{tables/rq2_lda}

\input{tables/rq2_lsi}

RQ2 asks how well a simulation of using a topic model would perform as
it were to be used in real-time.
This is a much closer evaluation of a feature location technique to it
being used in an actual development environment.
We also wanted to know if there were any differences of performance
between two popular algorithms, LSI and LDA.
Table~\ref{table:rq2:class:lda} and Table~\ref{table:rq2:class:lsi}
summarize the results of each subject system when
evaluated at the class granularity with LDA and LSI, respectively.
Similarly, Table~\ref{table:rq2:method:lda} and Table~\ref{table:rq2:method:lsi}
summarize the results of each subject system when
evaluated at the method granularity.
In each of the tables, we bold which of the two MRRs is greater,
and denote which $p$-values were calculated on sample sizes $N<10$ with an asterisk.

For temporal LDA at the class-level we note an improvement in MRR for 11 of the 15 systems
with 5 of these 11 systems statistically significant at $p<0.01$.
None of the 4 systems with MRR in favor of batch changesets were statistically significant.
Likewise, for the temporal LSI evaluations at class granularity, 14 of the 15 systems improve
with 11 of the systems statistically significant.
Overall, temporal evaluation performs significantly better than batch under both LDA and LSI.


For temporal LDA at the method-level we note an improvement in MRR for 4 of the 6 systems
with 2 of the 4 systems statistically significant at $p<0.01$.
Likewise, for the LSI evaluations at the method-level, 2 of the 6 systems improve
with statistical significance on only 1 of those 2 systems.
Overall, temporal evaluation performs significantly better than batch under LDA.
However, temporal LSI performs slightly worse than batch with statistical significance.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Discussion}

The results outlined in the previous section warrants some qualitative discussion.
In particular, our analysis shows significant affects between
snapshots and changesets, and between snapshots and temporal evaluation.
However, the results are mixed between each and are not conclusive.

In this study, we've asked two research questions which lead to two comparisons.
First, we compare a batch topic-modeling-based FLT trained on the changesets
of a project's history to one trained on the snapshot of source code entities.
Second, we compare a batch topic-modeling-based FLT trained on changesets
to a temporal topic-modeling-based FLT trained on the same changesets over time.
Our results are mixed, hence we end up with four possible situations:

\begin{enumerate}
    \item Changesets in batch are better than shapshots in batch \emph{and}
        changesets in batch are better than changesets temporally
    \item Changesets in batch are better than snapshots in batch \emph{and}
        changesets temporally are better than changesets in batch
    \item Snapshots in batch are better than changesets in batch \emph{and}
        changesets in batch are better than changesets temporally
    \item Snapshots in batch are better than changesets in batch \emph{and}
        changesets temporally are better than changesets in batch
\end{enumerate}

We will now discuss each of these situations in detail.

\todo{This section needs to be re-written}

% Snapshots < Changeset && Batch > Temporal
%  LDA Method:
%     argouml 22
%     argouml 24
%  LDA Class:
%     argouml 22
%     bookkeeper
%     mahout
%     zookeeper
%  LSI Class:
%     bookkeeper
%  LSI method:
%     jedit

Situation \#1 occurs in
4 out of 15 systems for LDA at the class-level,
1 out of 15 systems for LSI at the class-level,
2 out of 6 systems for LDA at the method-level,
and 1 out of 6 systems for LSI at the method-level.
We hypothesize that this is due to the nature of the batch evaluation versus the temporal evaluation.
In the batch evaluation, the model is trained on all data before being queried,
while in the temporal evaluation the model is trained on partial data before being queried.
This allows for the batch model to be more accurate because it is trained on more data
and reveals feature location research evaluations may not be accurately portraying
how an FLT would perform in a real scenario.

% Snapshots < Changeset && Batch < Temporal
%  LDA Method:
%     mucommander
%  LDA Class:
%     derby 109
%     openjpa 201
%     openjpa 220
%     tika
%  LSI class:
%     argouml 22
%     argouml 24
%     argouml 262
%     derby 109
%     jabref
%     mahout
%     mucommander
%     tika
%  LSI method:
%     mucommander
%
Situation \#2 occurs in
4 out of 15 systems for LDA at the class-level,
8 out of 15 systems for LSI at the class-level,
1 out of 6 systems for LDA at the method-level,
and 1 out of 6 systems for LSI at the method-level.
We hypothesize that this is due to the same previous reason, but that
temporal evaluation more accurately captures the correct state of the system (i.e., the source code entities)
at the point in time when querying is done.
Since querying on the batch models is after the model is completely trained,
there may be source code entities that do not exist in the system anymore
that were at one time changed to complete a certain task.
Again, the temporal evaluation better captures this scenario.

%  Snapshots > Changeset && Batch < Temporal
%  LDA Method:
%     argouml 262
%     jabref
%     jedit
%  LDA Class:
%     argouml 24
%     argouml 262
%     derby 107
%     jabref
%     jedit
%     mucommander
%     solr
%  LSI class:
%     derby 107
%     jedit
%     openjpa 201
%     openjpa 220
%     solr
%     zookeeper
%  LSI method:
%     argouml 22
%     argouml 24
%     argouml 262
%     jabref

Situation \#3 occurs in
7 out of 15 systems for LDA at the class-level,
6 out of 15 systems for LSI at the class-level,
3 out of 6 systems for LDA at the method-level,
and 4 out of 6 systems for LSI at the method-level.
Similarly, this could be because of how the models are trained.
Although the batch changesets performed worse in both cases, it does
improve during temporal evaluation.
This does not mean that changesets are bad, but more accurately model
the system over time.

% Snapshots > Changeset && Batch > Temporal
We note that Situation \#4 never occurs.
This also supports the hypothesis that temporal evaluation more accurately portrays the system over time.
However, we cannot conclude this without also evaluating snapshot-based topic models temporally.
