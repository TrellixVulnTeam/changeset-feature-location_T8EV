% vim:syntax=tex

In this section we provide an overview of the document extraction and retreival process
used for feature location.
We also provide an overview of two topic models,
latent semantic indexing (LSI) and latent Dirichlet allocation (LDA),
and review closely related work.

\subsection{Document Extraction and Retrieval Process}


\begin{figure*}
\vspace{2mm}
\centerline{\includegraphics[width=.8625\textwidth]{figures/Process}}
\caption{The extraction and retrieval process}
\label{fig:process}
\vspace{-2mm}
\end{figure*}

We use the following terminology to describe document extraction of source code.
A \textit{word} is the basic unit of discrete data in a software lexicon and is a sequence of letters.
A \textit{token} is a sequence of non-whitespace characters containing one or more words.
An \textit{entity} is a named source element such as a method,
and an \textit{identifier} is a token representing the name of an entity.
\textit{Comments} and \textit{literals} are sequences of tokens delimited by language-specific markers (e.g., /* */ and quotes).
The \textit{document} which corresponds to a class is a sequence of words $d = (w_1, \ldots, w_m)$,
and a \textit{corpus} is a set of documents (i.e., classes) $D = (d_1, \ldots, d_n)$.

The left side of Figure~\ref{fig:process} illustrates the document extraction process.
A document extractor takes source code as input and produces a corpus as output.
Each document in the corpus contains the words associated with a class.
The text extractor is the first part of the document extractor.
It parses the source code and produces a token stream for each class.
The preprocessor is the second part of the document extractor.
It applies a series of transformations to each token and
produces one or more words from the token.
The transformations~\cite{Marcus-etal:2004,Marcus-Menzies:2010}: % that we use are:
\begin{itemize}
    \item {\it Splitting}: separate tokens into constituent words
        based on common coding style conventions (e.g., the use of camel case or underscores)
        and on the presence of non-letters (e.g., punctuation or digits)
    \item {\it Normalizing}: replace each upper case letter with the corresponding
        lower case letter
    \item {\it Filtering}: remove common words such as articles (e.g., `an' or `the'),
        programming language keywords, standard library entity names, or short words
\end{itemize}

The right side of Figure~\ref{fig:process} illustrates the retrieval process.
Once a topic model has been built, it can be used as part of a query engine.
The query engine's job is to take a query and infer how it relates to topics in the model.
Additionally, it can classify, or rank, the results of this inference to another inference.
That is, the classifier step finds other documents that have similar topics in the model.

\subsection{Latent Semantic Indexing}

Latent semantic indexing~\cite{Deerwester:1990} is an indexing and
retrieval methodolgy. LSI uses a statistical technique, singular value
decomposition to identify patterns within the unstructured data. That is,
LSI identifies relationships between terms and documents, and places
documents that are related close to one another creating a semantic space.


\subsection{Latent Dirichlet Allocation}

Latent Dirichlet allocation~\cite{Blei-etal:2003} is a generative topic model.
LDA models each document in a corpus of discrete data as a finite mixture over a set of topics
and models each topic as an infinite mixture over a set of topic probabilities.
That is, LDA models each document as a probability distribution
indicating the likelihood that it expresses each topic and
models each topic that it infers as a probability distribution
indicating the likelihood of a word from the corpus being assigned to the topic.

Inputs to LDA include a corpus and $K$, the number of topics.
LDA represents each document in the corpus as a bag-of-word (multiset)
and thus disregards word order and structure.
Outputs of LDA include $\phi$, the term-topic probability distribution,
and $\theta$, the topic-document probability distribution.


\subsection{Feature Location}

