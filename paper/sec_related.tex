% vim:syntax=tex

In this section we provide an overview of two topic models,
latent semantic indexing (LSI) and latent Dirichlet allocation (LDA),
and review closely related work.

\subsection{Latent Semantic Indexing}

Latent semantic indexing~\cite{Deerwester:1990} is an indexing and 
retrieval methodolgy. LSI uses a statistical technique, singular value 
decomposition to identify patterns within the unstructured data. That is, 
LSI identifies relationships between terms and documents, and places 
documents that are related close to one another creating a semantic space. 


\subsection{Latent Dirichlet Allocation}

Latent Dirichlet allocation~\cite{Blei-etal:2003} is a generative topic model.
LDA models each document in a corpus of discrete data as a finite mixture over a set of topics
and models each topic as an infinite mixture over a set of topic probabilities.
That is, LDA models each document as a probability distribution
indicating the likelihood that it expresses each topic and
models each topic that it infers as a probability distribution
indicating the likelihood of a word from the corpus being assigned to the topic.

Inputs to LDA include a corpus and $K$, the number of topics.
LDA represents each document in the corpus as a bag-of-word (multiset)
and thus disregards word order and structure.
Outputs of LDA include $\phi$, the term-topic probability distribution,
and $\theta$, the topic-document probability distribution.


\subsection{Feature Location}

