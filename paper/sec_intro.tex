% vim:syntax=tex

Feature location is a frequent and fundamental activity for a developer tasked
with changing a software system. Whether a change task involves adding,
modifying, or removing a feature, a developer cannot complete the task without
first locating the source code that implements the feature. The
state-of-the-practice in feature location is to use an IDE tool based on keyword
or regex search, but Ko et al.~\cite{Ko-etal_2006} observed such tools leading
developers to failed searches nearly 90\% of the time.

The state-of-the-art in feature location~\cite{Dit-etal_2013a} is to use a
feature location technique (FLT) based, at least in part, on text retrieval
(TR). The standard methodology~\cite{Marcus-etal_2004} is to extract a document
for each class or method in a source code snapshot, to train a TR model on those
documents, and to create an index of the documents from the trained model.
Topics models (TMs)~\cite{Blei_2012} such as latent Dirichlet allocation
(LDA)~\cite{Blei-etal_2003} are the state-of-the-art in TR and outperform
vector-space models (VSMs) in the contexts of natural
language~\cite{Deerwester-etal_1990,Blei-etal_2003} and source
code~\cite{Poshyvanyk-etal_2007,Lukins-etal_2010}. Yet, modern TMs such as
online LDA~\cite{Hoffman-etal_2010} natively support only the online addition of
a new document, whereas VSMs also natively support online modification or
removal of an existing document. So, TM-based FLTs provide the best accuracy,
but unlike VSM-based FLTs, they require computationally-expensive retraining
subsequent to source code changes.

Rao\cite{Rao_2013} proposed FLTs based on customizations of LDA and latent
semantic indexing (LSI) that support online modification and removal.
These FLTs require less-frequent retraining than other TM-based FLTs, but the
remaining cost of periodic retraining inhibits their application to large
software, and the reliance on customization hinders their extension to new TMs.

We envision an FLT that is: (1)~accurate like a TM-based FLT, (2)~inexpensive to
update like a VSM-based FLT, and (3)~extensible to accommodate any off-the-shelf
TR model that supports online addition of a new document. Unfortunately, our
vision is incompatible with the standard methodology for FLTs. Existing
VSM-based FLTs fail to satisfy the first criteria, and existing TM-based FLTs
fail to satisfy the second or third criteria. Indeed, given the current
state-of-the-art in TR, it is impossible for a FLT to satisfy all three criteria
while following the standard methodology.

\todo{R1: cleanup this para}
\todo{R3: unlear how we satisfied item 1}
In this paper we propose a new methodology for FLTs. Our methodology is to
extract a document for each changeset in the source code history and to train a
TR model on the changeset documents, and then to extract a document for each
class or method in a source code snapshot and to create an index of the
class/method documents from the trained (changeset) model. This new methodology
stems from four key observations:
\begin{itemize}[leftmargin=*]
  \item
    Like a class/method definition, a changeset has program text.
  \item
    Unlike a class/method definition, a changeset is immutable.
  \item
    A changeset corresponds to a commit.
  \item
    An atomic commit involves a single feature.
\end{itemize}
It follows from the first two observations that it is possible for an FLT
following our methodology to satisfy all three of the criteria above. The next
two observations influence the training and indexing steps of our methodology,
which have the conceptual effect of relating classes (or methods) to changeset
topics. By contrast, the training and indexing steps of the standard methodology
have the conceptual effect of relating classes to class topics (or methods to
method topics).

To evaluate the new methodology, we used it to implement FLTs based on online
LSI and online LDA. We next used two benchmarks to compare the accuracy of these
FLTs to the accuracy of analogous FLTs following the standard methodology.
Combined, the two benchmarks comprise over 600 defects and features from 4
open-source Java projects with both method- and class-level goldsets. Our
evaluation results provide evidence that our new methodology is sound and that
following it yields FLTs with similar accuracy to those following the standard
methodology, but without the retraining costs.

The remainder of the paper is organized as follows. We first review background
and related work (\S\ref{sec:related}) We next present our new methodology for
FLTs (\S\ref{sec:changeset}) and report evaluation results for the
online-LDA-based FLT (\S\ref{sec:study}). We then conclude
(\S\ref{sec:conclusion}).
