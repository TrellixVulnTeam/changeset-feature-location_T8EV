% vim:syntax=tex

Feature location is a frequent and fundamental activity for a developer tasked with changing a software system.
Whether a change task involves adding, modifying, or removing a feature, a developer cannot complete the change task before identifying the source code that implements the feature, and due to the scale of modern software systems, the developer cannot identify that source code manually.
Consequently, feature location is essential to software maintenance and evolution, and automated feature location techniques (FLTs) are essential to developers.
Given that the current state-of-the-practice for feature location is to use a keyword-search-based tool such as \texttt{grep}, and that developers fail about 90\% of the time when using such a tool~\cite{Ko-etal:2006}, the potential for FLT research to impact practice is evident.
Unfortunately, recent FLTs suffer from key limitations that hinder their potential adoption in and impact on practice.

Recent FLTs are based on different combinations of dynamic, static, textual, and historical analysis, but most are based (at least in part) on textual analysis of source code~\cite{Dit-etal:2011}.
Each of these FLTs relies on a text retrieval (TR) model such as a vector space model (VSM), latent semantic indexing (LSI)~\cite{Deerwester-etal:1990}, or latent Dirichlet allocation (LDA)~\cite{Blei-etal:2003}, and all of these FLTs share a common methodology in which a TR model is trained on a source code snapshot and an index is created from the trained model.
Within this snapshot-based methodology, for an FLT to maintain its accuracy, its underlying TR model (and index) must be updated when the source code is changed.
That is, for an FLT to provide accurate results as its snapshot becomes obsolete, the TR model must be retrained (and the index recreated), either incrementally (from the changesets) or completely (from the current snapshot).
%This need for incremental or complete retraining of the TR model in response to source code changes is the source of the two major limitations that affect recent TR-based FLTs.

Complete retraining of a TR model can be computationally expensive, particularly for a large software system.
Therefore, an FLT that requires complete retraining in response to source code changes is not a viable option for inclusion in an IDE.
However, while many online TR models permit continual addition of new documents, few permit modification or removal of existing documents.
In particular, of the TR models on which FLTs from the literature are based, only VSMs permit incremental retraining in cases of modification to or removal of an existing document,
whereas LSI and LDA limit incremental retraining to cases of addition of a new document.
Because studies indicate that LSI and LDA are superior to a VSM, both in general~\cite{} and in the context of source code search~\cite{},
Rao~\cite{Rao:2013} extended LSI and LDA as part of an incremental FLT framework that requires complete retraining only periodically.
Although the framework reduces retraining costs, it does not eliminate them completely, nor does it accept new TR models off the shelf (i.e., they must be extended).

%Researchers typically evaluate TR-based FLTs in batch mode by issuing multiple queries (corresponding to multiple features of interest) to a single index (corresponding to a single source code snapshot of interest).

Fresh take\ldots

Contributions\ldots




\begin{comment}
Software developers are often confronted with maintenance tasks that involve
navigation of repositories that preserve vast amounts of project history.
Navigating these software repositories can be a time-consuming task, because
their organization can be difficult to understand.  A software developer who is
tasked with changing a large software system spends effort on program
comprehension activities to gain the knowledge needed to make the
change~\cite{Corbi:1989}.  Fortunately, topic models such as latent Dirichlet
allocation (LDA)~\cite{Blei-etal:2003} can help developers to navigate and
understand software repositories by discovering topics (word distributions) that
reveal the thematic structure of the
data~\cite{Linstead-etal:2007,Thomas-etal:2011,Hindle-etal:2014}.

One particular application of topic models is for \emph{feature location}.
Feature location is the act of identifying the source code that implements
a system feature.  The current state-of-the-practice for feature location is to
use a keyword search tool, such as \texttt{grep}.  Ko et al.~\cite{Ko-etal:2006}
show that developers fail using this type of searching upwards to 88\% of the
time.  Text retrieval techniques, such as topic modeling, show promise in
remedying this problem~\cite{Marcus-etal:2004}.

Typical topic-modeling-based feature location techniques (FLT) construct models
from corpora of text extracted from a source code
snapshot~\cite{Dit-etal:2011}.  To use a topic-modeling-based FLT, there are
generally two key steps: training and indexing.  In the first step, a corpus of
source code entities, such as methods or classes, are used to train the model to
learn word co-occurences within those entities.  The indexing step uses the
trained model to construct an index of the source code entities based on their
inferred topic distribution.  That is, an index is made of each source code's
\emph{thematic structure}, and not it's raw content.  Keeping such a model and
index up-to-date is expensive, because the frequency and scope of source code
changes, such as file removal, necessitate retraining the model on the updated
corpus and reindexing.  This situation is sub-optimal whether your perspective
is academic research or industrial tool-building.  Like Rao et
al.~\cite{Rao-etal:2013}, our primary research goal is elimination of this cost.
However, unlike Rao et al., we do not intend to develop new topic modeling
techniques, but rather use the existing ones.

In this paper, we propose a fresh take on topic-modeling-based FLTs by
leveraging online topic models and mining software repositories to construct
topic models that do not need retraining.  Online topic models do not need to
know the entire input corpus prior to
training~\cite{Hoffman-etal:2010}.  That is, online topic models can
be incrementally trained over time as more data becomes available.
Moreover, a version control repository, such as Git, keeps a history of source
code documents as they change over time.  These changes are represented as
changesets, which provide concise views of the differences between two revisions
of the same document.  By training an online topic model on changesets and
indexing the source code on that model, we can stream documents (i.e.,
changesets) from the version control repository to incrementally train the topic
model.  This enables searching over the current source code index without
retraining an entirely new model.

In our previous work~\cite{Corley-etal:2014}, we show that topic models trained
on changesets produce topics which have comparable topic distinctness
scores~\cite{Thomas-etal:2011} as topic models trained on snapshots.  Further,
we show that the corpora express the same frequency of words.  We expand the
work to demonstrate the effectiveness of changeset topic modeling for feature
location and report on an empirical study in which we investigate the
feasibility of this approach.
We define a LDA-based FLT using changesets.  We combine two benchmarks totaling
over 1200 defects and features from fourteen open source Java projects.  We also
present a \emph{temporal simulation} that approximates how the FLT would perform
throughout the evolution of a project.

Our results show that the changeset approach is feasible and has performace
comparable to the snapshot approach.  In many cases the changeset approach
out-performs current snapshot approaches, but is no silver bullet.  We argue
that the evidence suggests that changeset-based topic modeling warrants further
investigation and adoption.  Additionally, the temporal simulation suggests that
current evaluation approaches do not accurately capture the true FLT
performance.

This paper makes the following contributions:

\begin{itemize} \item An approach for using changesets for feature location
        \item A empirical study of fourteen open source Java projects \item
            Towards increasing open science principles in software engineering:
            the complete project history, source code, and an updated dataset
            for replication of this study.  \end{itemize}

The remainder of the paper is organized as follows.  We first review background
and related work (\S\ref{sec:related}) before introducing our new
changeset-based FLT (\S\ref{sec:changeset}).  We next discuss our case study
(\S\ref{sec:study}), which spans fourteen open source Java projects.  We then
conclude (\S\ref{sec:conclusion}).
\end{comment}

